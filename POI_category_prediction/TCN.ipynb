{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN text classification model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25161, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>clean_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>no_of_ratings</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>type_1</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Colombo Swimming Club</td>\n",
       "      <td>colombo swimming club</td>\n",
       "      <td>ChIJETFLQ0FZ4joRwioIp4MnzHA</td>\n",
       "      <td>148 “Storm Lodge” Galle Road 3</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>tourist_attraction</td>\n",
       "      <td>recreational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Storm lodge</td>\n",
       "      <td>storm lodge</td>\n",
       "      <td>ChIJGSU_sGtZ4joRiyZjgglUlPE</td>\n",
       "      <td>148 “Storm Lodge” Galle Road 3, Colombo 00300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lodging</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ayura</td>\n",
       "      <td>ayura</td>\n",
       "      <td>ChIJQwy6bEFZ4joRU9m1ylxsbMs</td>\n",
       "      <td>142, Yathama Building, Galle Road, Colombo</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>jewelry_store</td>\n",
       "      <td>shopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Baby Gallery Kurulapina</td>\n",
       "      <td>baby gallery kurulapina</td>\n",
       "      <td>ChIJR1dqFUFZ4joRsqACvaqRNws</td>\n",
       "      <td>WR8X+655, Colombo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clothing_store</td>\n",
       "      <td>shopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Galle Face , Taj Hotel</td>\n",
       "      <td>galle face taj hotel</td>\n",
       "      <td>ChIJo015E0FZ4joRdGUr8gWxo4E</td>\n",
       "      <td>138 Colombo - Galle Main Road, Colombo</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>dining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44395</th>\n",
       "      <td>Ichiro Motoring - Wijerama</td>\n",
       "      <td>ichiro motoring wijerama</td>\n",
       "      <td>ChIJk3QsSX1a4joRDO-HD30GZT8</td>\n",
       "      <td>High Level Road, Nugegoda</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>car_repair</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44396</th>\n",
       "      <td>Softlogic Glomark Supermarket</td>\n",
       "      <td>softlogic glomark supermarket</td>\n",
       "      <td>ChIJD436Ynxb4joR_tn_grvgpNs</td>\n",
       "      <td>Nugegoda Delkanda Apartment, High Level Road, ...</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>grocery_or_supermarket</td>\n",
       "      <td>shopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44397</th>\n",
       "      <td>Softlogic Glomark Supermarket</td>\n",
       "      <td>softlogic glomark supermarket</td>\n",
       "      <td>ChIJD436Ynxb4joR_tn_grvgpNs</td>\n",
       "      <td>Nugegoda Delkanda Apartment, High Level Road, ...</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>grocery_or_supermarket</td>\n",
       "      <td>shopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44398</th>\n",
       "      <td>Mirai Auto Land (Pvt) Ltd</td>\n",
       "      <td>mirai auto land pvt ltd</td>\n",
       "      <td>ChIJ6V8_E2da4joREMxUiWN0uVA</td>\n",
       "      <td>Delkanda, 383 High Level Rd, Avissawella Road,...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>car_repair</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44399</th>\n",
       "      <td>Colombo Swimming Club</td>\n",
       "      <td>colombo swimming club</td>\n",
       "      <td>ChIJETFLQ0FZ4joRwioIp4MnzHA</td>\n",
       "      <td>148 “Storm Lodge” Galle Road 3</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>tourist_attraction</td>\n",
       "      <td>recreational</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25161 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                name                     clean_name  \\\n",
       "0              Colombo Swimming Club          colombo swimming club   \n",
       "1                        Storm lodge                    storm lodge   \n",
       "5                              Ayura                          ayura   \n",
       "6            Baby Gallery Kurulapina        baby gallery kurulapina   \n",
       "7             Galle Face , Taj Hotel           galle face taj hotel   \n",
       "...                              ...                            ...   \n",
       "44395     Ichiro Motoring - Wijerama       ichiro motoring wijerama   \n",
       "44396  Softlogic Glomark Supermarket  softlogic glomark supermarket   \n",
       "44397  Softlogic Glomark Supermarket  softlogic glomark supermarket   \n",
       "44398      Mirai Auto Land (Pvt) Ltd        mirai auto land pvt ltd   \n",
       "44399          Colombo Swimming Club          colombo swimming club   \n",
       "\n",
       "                          place_id  \\\n",
       "0      ChIJETFLQ0FZ4joRwioIp4MnzHA   \n",
       "1      ChIJGSU_sGtZ4joRiyZjgglUlPE   \n",
       "5      ChIJQwy6bEFZ4joRU9m1ylxsbMs   \n",
       "6      ChIJR1dqFUFZ4joRsqACvaqRNws   \n",
       "7      ChIJo015E0FZ4joRdGUr8gWxo4E   \n",
       "...                            ...   \n",
       "44395  ChIJk3QsSX1a4joRDO-HD30GZT8   \n",
       "44396  ChIJD436Ynxb4joR_tn_grvgpNs   \n",
       "44397  ChIJD436Ynxb4joR_tn_grvgpNs   \n",
       "44398  ChIJ6V8_E2da4joREMxUiWN0uVA   \n",
       "44399  ChIJETFLQ0FZ4joRwioIp4MnzHA   \n",
       "\n",
       "                                                vicinity  no_of_ratings  \\\n",
       "0                         148 “Storm Lodge” Galle Road 3         1181.0   \n",
       "1          148 “Storm Lodge” Galle Road 3, Colombo 00300            NaN   \n",
       "5             142, Yathama Building, Galle Road, Colombo           16.0   \n",
       "6                                      WR8X+655, Colombo            NaN   \n",
       "7                 138 Colombo - Galle Main Road, Colombo            2.0   \n",
       "...                                                  ...            ...   \n",
       "44395                          High Level Road, Nugegoda           60.0   \n",
       "44396  Nugegoda Delkanda Apartment, High Level Road, ...         1278.0   \n",
       "44397  Nugegoda Delkanda Apartment, High Level Road, ...         1278.0   \n",
       "44398  Delkanda, 383 High Level Rd, Avissawella Road,...           52.0   \n",
       "44399                     148 “Storm Lodge” Galle Road 3         1181.0   \n",
       "\n",
       "       avg_rating                  type_1       purpose  \n",
       "0             4.4      tourist_attraction  recreational  \n",
       "1             NaN                 lodging      personal  \n",
       "5             4.5           jewelry_store      shopping  \n",
       "6             NaN          clothing_store      shopping  \n",
       "7             5.0              restaurant        dining  \n",
       "...           ...                     ...           ...  \n",
       "44395         3.5              car_repair      personal  \n",
       "44396         4.2  grocery_or_supermarket      shopping  \n",
       "44397         4.2  grocery_or_supermarket      shopping  \n",
       "44398         4.4              car_repair      personal  \n",
       "44399         4.4      tourist_attraction  recreational  \n",
       "\n",
       "[25161 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"places_names_labelled.csv\", index_col = 0)\n",
    "# corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['purpose'].replace(corpus['purpose'].unique(), list(range(0, len(corpus['purpose'].unique()))), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(corpus,corpus['purpose'],test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJV8dGfzeAxX"
   },
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>## Develop Vocabulary\n",
    "\n",
    "A part of preparing text for text classification involves defining and tailoring the vocabulary of words supported by the model. **We can do this by loading all of the documents in the dataset and building a set of words.**\n",
    "\n",
    "The larger the vocabulary, the more sparse the representation of each word or document. So, we may decide to support all of these words, or perhaps discard some. The final chosen vocabulary can then be saved to a file for later use, such as filtering words in new documents in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1613881803995,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "8ozAFDk3eAxe"
   },
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1650,
     "status": "ok",
     "timestamp": 1613881808491,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "fWW5xE8peAxl",
    "outputId": "e29c9383-98b8-4e99-b2f3-85f7c62fbda1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  wycherley montessori\n",
      "Into a sequence of int: [1398, 122]\n",
      "Into a padded sequence: [1398  122    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "# X_train,X_test,y_train,y_test\n",
    "# Separate the sentences and the labels\n",
    "train_x = list(X_train.clean_name)\n",
    "train_y = np.array(X_train.purpose)\n",
    "test_x = list(X_test.clean_name)\n",
    "test_y = np.array(X_test.purpose)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "print(\"Example of sentence: \", train_x[4])\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1586,
     "status": "ok",
     "timestamp": 1613881812377,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "pO2INZaHeAxm",
    "outputId": "09eb9a33-b69b-432e-c86a-5e63d59e022c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "ltd 2\n",
      "pvt 3\n",
      "lanka 4\n",
      "the 5\n",
      "sri 6\n",
      "colombo 7\n",
      "of 8\n",
      "and 9\n",
      "house 10\n",
      "13138\n"
     ]
    }
   ],
   "source": [
    "# See the first 10 words in the vocabulary\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN Model\n",
    "\n",
    "Now, we will build Temporal Convolutional Network (CNN) models to classify encoded documents as either positive or negative.\n",
    "\n",
    "The model takes inspiration from https://github.com/philipperemy/keras-tcn and https://www.kaggle.com/christofhenkel/temporal-convolutional-network\n",
    "\n",
    "__Arguments__\n",
    "`TCN(nb_filters=64, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16, 32], padding='causal', use_skip_connections=False, dropout_rate=0.0, return_sequences=True, activation='relu', kernel_initializer='he_normal', use_batch_norm=False, **kwargs)`\n",
    "\n",
    "- `nb_filters`: Integer. The number of filters to use in the convolutional layers. Would be similar to units for LSTM.\n",
    "- `kernel_size`: Integer. The size of the kernel to use in each convolutional layer.\n",
    "- `dilations`: List. A dilation list. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "- `nb_stacks`: Integer. The number of stacks of residual blocks to use.\n",
    "- `padding`: String. The padding to use in the convolutions. 'causal' for a causal network (as in the original implementation) and - `'same' for a non-causal network.\n",
    "- `use_skip_connections`: Boolean. If we want to add skip connections from input to each residual block.\n",
    "- `return_sequences`: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "- `dropout_rate`: Float between 0 and 1. Fraction of the input units to drop.\n",
    "- `activation`: The activation used in the residual blocks o = activation(x + F(x)).\n",
    "- `kernel_initializer`: Initializer for the kernel weights matrix (Conv1D).\n",
    "- `use_batch_norm`: Whether to use batch normalization in the residual layers or not.\n",
    "- `kwargs`: Any other arguments for configuring parent class Layer. For example \"name=str\", Name of the model. Use unique names when using multiple TCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define our TCN model as follows:\n",
    "- One TCN layer with 100 filters, kernel size 1-6, and relu and tanh activation function;\n",
    "- Dropout size = 0.5;\n",
    "- Optimizer: Adam (The best learning algorithm so far)\n",
    "- Loss function: binary cross-entropy (suited for binary classification problem)\n",
    "\n",
    "**Note**: \n",
    "- The whole purpose of dropout layers is to tackle the problem of over-fitting and to introduce generalization to the model. Hence it is advisable to keep dropout parameter near 0.5 in hidden layers. \n",
    "- https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def define_model(kernel_size = 3, activation='relu', input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    inp = Input( shape=(max_length,))\n",
    "    x = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_length)(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = TCN(128,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn1')(x)\n",
    "    x = TCN(64,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn2')(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = Dense(16, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(8, activation=\"softmax\")(conc)    \n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile( loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 300)     300000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 100, 300)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tcn1 (TCN)                      (None, 100, 128)     400256      spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tcn2 (TCN)                      (None, 100, 64)      94656       tcn1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           tcn2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 64)           0           tcn2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2064        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 797,112\n",
      "Trainable params: 797,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model(input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tcn_full_summary(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class myCallback(tf.keras.callbacks.Callback):\n",
    "#     # Overide the method on_epoch_end() for our benefit\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if (logs.get('accuracy') > 0.93):\n",
    "#             print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "#             self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=20, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Training 1: tanh activation, 1 kernel size.\n",
      "-------------------------------------------\n",
      "Epoch 1/50\n",
      "403/403 [==============================] - 49s 123ms/step - loss: 1.1872 - accuracy: 0.5882 - val_loss: 0.7973 - val_accuracy: 0.7461\n",
      "Epoch 2/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.6163 - accuracy: 0.8115 - val_loss: 0.7590 - val_accuracy: 0.7598\n",
      "Epoch 3/50\n",
      "403/403 [==============================] - 47s 117ms/step - loss: 0.3940 - accuracy: 0.8810 - val_loss: 0.7835 - val_accuracy: 0.7421\n",
      "Epoch 4/50\n",
      "403/403 [==============================] - 50s 124ms/step - loss: 0.2913 - accuracy: 0.9131 - val_loss: 0.8390 - val_accuracy: 0.7363\n",
      "Epoch 5/50\n",
      "403/403 [==============================] - 50s 124ms/step - loss: 0.2251 - accuracy: 0.9340 - val_loss: 0.9005 - val_accuracy: 0.7546\n",
      "Epoch 6/50\n",
      "403/403 [==============================] - 53s 131ms/step - loss: 0.1887 - accuracy: 0.9445 - val_loss: 0.9465 - val_accuracy: 0.7510\n",
      "Epoch 7/50\n",
      "403/403 [==============================] - 55s 136ms/step - loss: 0.1652 - accuracy: 0.9499 - val_loss: 1.0120 - val_accuracy: 0.7445\n",
      "Epoch 8/50\n",
      "403/403 [==============================] - 53s 131ms/step - loss: 0.1505 - accuracy: 0.9563 - val_loss: 1.0608 - val_accuracy: 0.7324\n",
      "Epoch 9/50\n",
      "403/403 [==============================] - 51s 127ms/step - loss: 0.1364 - accuracy: 0.9588 - val_loss: 1.0995 - val_accuracy: 0.7572\n",
      "Epoch 10/50\n",
      "403/403 [==============================] - 48s 120ms/step - loss: 0.1150 - accuracy: 0.9640 - val_loss: 1.1029 - val_accuracy: 0.7530\n",
      "Epoch 11/50\n",
      "403/403 [==============================] - 49s 122ms/step - loss: 0.1088 - accuracy: 0.9668 - val_loss: 1.1776 - val_accuracy: 0.7516\n",
      "Epoch 12/50\n",
      "403/403 [==============================] - 57s 142ms/step - loss: 0.1085 - accuracy: 0.9666 - val_loss: 1.2059 - val_accuracy: 0.7403\n",
      "Epoch 13/50\n",
      "403/403 [==============================] - 53s 132ms/step - loss: 0.0976 - accuracy: 0.9697 - val_loss: 1.2101 - val_accuracy: 0.7502\n",
      "Epoch 14/50\n",
      "403/403 [==============================] - 53s 132ms/step - loss: 0.0943 - accuracy: 0.9699 - val_loss: 1.4043 - val_accuracy: 0.7504\n",
      "Epoch 15/50\n",
      "403/403 [==============================] - 52s 128ms/step - loss: 0.0982 - accuracy: 0.9693 - val_loss: 1.2472 - val_accuracy: 0.7451\n",
      "Epoch 16/50\n",
      "403/403 [==============================] - 52s 128ms/step - loss: 0.0881 - accuracy: 0.9728 - val_loss: 1.2423 - val_accuracy: 0.7318\n",
      "Epoch 17/50\n",
      "403/403 [==============================] - 53s 132ms/step - loss: 0.0761 - accuracy: 0.9765 - val_loss: 1.3581 - val_accuracy: 0.7437\n",
      "Epoch 18/50\n",
      "403/403 [==============================] - 54s 135ms/step - loss: 0.0789 - accuracy: 0.9759 - val_loss: 1.4487 - val_accuracy: 0.7481\n",
      "Epoch 19/50\n",
      "403/403 [==============================] - 53s 131ms/step - loss: 0.0752 - accuracy: 0.9759 - val_loss: 1.4784 - val_accuracy: 0.7568\n",
      "Epoch 20/50\n",
      "403/403 [==============================] - 50s 124ms/step - loss: 0.0709 - accuracy: 0.9772 - val_loss: 1.4422 - val_accuracy: 0.7367\n",
      "Epoch 21/50\n",
      "403/403 [==============================] - 51s 126ms/step - loss: 0.0662 - accuracy: 0.9777 - val_loss: 1.5084 - val_accuracy: 0.7449\n",
      "Epoch 22/50\n",
      "403/403 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9786Restoring model weights from the end of the best epoch.\n",
      "403/403 [==============================] - 53s 131ms/step - loss: 0.0689 - accuracy: 0.9786 - val_loss: 1.3908 - val_accuracy: 0.7491\n",
      "Epoch 00022: early stopping\n",
      "Test Accuracy: 75.97854137420654\n",
      "Done!\n",
      "\n",
      "  Activation Filters       Acc\n",
      "0       tanh       1  0.759785\n",
      "\n",
      "-------------------------------------------\n",
      "Training 2: tanh activation, 2 kernel size.\n",
      "-------------------------------------------\n",
      "Epoch 1/50\n",
      "403/403 [==============================] - 49s 122ms/step - loss: 1.2216 - accuracy: 0.5563 - val_loss: 0.8465 - val_accuracy: 0.7077\n",
      "Epoch 2/50\n",
      "403/403 [==============================] - 49s 122ms/step - loss: 0.6231 - accuracy: 0.8089 - val_loss: 0.7737 - val_accuracy: 0.7560\n",
      "Epoch 3/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.3742 - accuracy: 0.8932 - val_loss: 0.8062 - val_accuracy: 0.7602\n",
      "Epoch 4/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.2713 - accuracy: 0.9219 - val_loss: 0.8860 - val_accuracy: 0.7562\n",
      "Epoch 5/50\n",
      "403/403 [==============================] - 48s 120ms/step - loss: 0.2154 - accuracy: 0.9378 - val_loss: 0.9060 - val_accuracy: 0.7532\n",
      "Epoch 6/50\n",
      "403/403 [==============================] - 49s 120ms/step - loss: 0.1845 - accuracy: 0.9458 - val_loss: 0.9362 - val_accuracy: 0.7580\n",
      "Epoch 7/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.1561 - accuracy: 0.9547 - val_loss: 0.9634 - val_accuracy: 0.7630\n",
      "Epoch 8/50\n",
      "403/403 [==============================] - 49s 122ms/step - loss: 0.1403 - accuracy: 0.9577 - val_loss: 0.9803 - val_accuracy: 0.7594\n",
      "Epoch 9/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.1291 - accuracy: 0.9616 - val_loss: 1.0361 - val_accuracy: 0.7348\n",
      "Epoch 10/50\n",
      "403/403 [==============================] - 51s 126ms/step - loss: 0.1207 - accuracy: 0.9644 - val_loss: 1.1684 - val_accuracy: 0.7312\n",
      "Epoch 11/50\n",
      "403/403 [==============================] - 54s 133ms/step - loss: 0.1198 - accuracy: 0.9645 - val_loss: 1.0788 - val_accuracy: 0.7534\n",
      "Epoch 12/50\n",
      "403/403 [==============================] - 54s 134ms/step - loss: 0.1064 - accuracy: 0.9697 - val_loss: 1.1205 - val_accuracy: 0.7556\n",
      "Epoch 13/50\n",
      "403/403 [==============================] - 52s 128ms/step - loss: 0.0968 - accuracy: 0.9706 - val_loss: 1.2025 - val_accuracy: 0.7497\n",
      "Epoch 14/50\n",
      "403/403 [==============================] - 54s 133ms/step - loss: 0.0899 - accuracy: 0.9741 - val_loss: 1.3299 - val_accuracy: 0.7544\n",
      "Epoch 15/50\n",
      "403/403 [==============================] - 54s 135ms/step - loss: 0.0905 - accuracy: 0.9746 - val_loss: 1.2811 - val_accuracy: 0.7530\n",
      "Epoch 16/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.0806 - accuracy: 0.9753 - val_loss: 1.3123 - val_accuracy: 0.7504\n",
      "Epoch 17/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.0804 - accuracy: 0.9746 - val_loss: 1.3366 - val_accuracy: 0.7493\n",
      "Epoch 18/50\n",
      "403/403 [==============================] - 48s 120ms/step - loss: 0.0767 - accuracy: 0.9764 - val_loss: 1.3721 - val_accuracy: 0.7502\n",
      "Epoch 19/50\n",
      "403/403 [==============================] - 48s 120ms/step - loss: 0.0760 - accuracy: 0.9759 - val_loss: 1.4009 - val_accuracy: 0.7469\n",
      "Epoch 20/50\n",
      "403/403 [==============================] - 49s 121ms/step - loss: 0.0730 - accuracy: 0.9768 - val_loss: 1.4236 - val_accuracy: 0.7473\n",
      "Epoch 21/50\n",
      "403/403 [==============================] - 50s 125ms/step - loss: 0.0665 - accuracy: 0.9792 - val_loss: 1.4422 - val_accuracy: 0.7487\n",
      "Epoch 22/50\n",
      "403/403 [==============================] - 55s 136ms/step - loss: 0.0686 - accuracy: 0.9792 - val_loss: 1.4228 - val_accuracy: 0.7510\n",
      "Epoch 23/50\n",
      "403/403 [==============================] - 51s 127ms/step - loss: 0.0690 - accuracy: 0.9788 - val_loss: 1.3803 - val_accuracy: 0.7518\n",
      "Epoch 24/50\n",
      "403/403 [==============================] - 53s 131ms/step - loss: 0.0590 - accuracy: 0.9806 - val_loss: 1.3803 - val_accuracy: 0.7467\n",
      "Epoch 25/50\n",
      "403/403 [==============================] - 51s 127ms/step - loss: 0.0623 - accuracy: 0.9795 - val_loss: 1.5223 - val_accuracy: 0.7520\n",
      "Epoch 26/50\n",
      "403/403 [==============================] - 53s 131ms/step - loss: 0.0535 - accuracy: 0.9831 - val_loss: 1.6254 - val_accuracy: 0.7457\n",
      "Epoch 27/50\n",
      "403/403 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9807Restoring model weights from the end of the best epoch.\n",
      "403/403 [==============================] - 52s 128ms/step - loss: 0.0590 - accuracy: 0.9807 - val_loss: 1.5151 - val_accuracy: 0.7528\n",
      "Epoch 00027: early stopping\n",
      "Test Accuracy: 76.29644274711609\n",
      "Done!\n",
      "\n",
      "  Activation Filters       Acc\n",
      "0       tanh       1  0.759785\n",
      "1       tanh       2  0.762964\n",
      "\n",
      "-------------------------------------------\n",
      "Training 3: tanh activation, 3 kernel size.\n",
      "-------------------------------------------\n",
      "Epoch 1/50\n",
      "403/403 [==============================] - 51s 126ms/step - loss: 1.2858 - accuracy: 0.5140 - val_loss: 0.8458 - val_accuracy: 0.7246\n",
      "Epoch 2/50\n",
      "403/403 [==============================] - 58s 143ms/step - loss: 0.7184 - accuracy: 0.7588 - val_loss: 0.7875 - val_accuracy: 0.7457\n",
      "Epoch 3/50\n",
      "403/403 [==============================] - 55s 136ms/step - loss: 0.4775 - accuracy: 0.8487 - val_loss: 0.7959 - val_accuracy: 0.7411\n",
      "Epoch 4/50\n",
      "403/403 [==============================] - 55s 136ms/step - loss: 0.3562 - accuracy: 0.8904 - val_loss: 0.8755 - val_accuracy: 0.7528\n",
      "Epoch 5/50\n",
      "403/403 [==============================] - 54s 134ms/step - loss: 0.2830 - accuracy: 0.9197 - val_loss: 0.9136 - val_accuracy: 0.7467\n",
      "Epoch 6/50\n",
      "403/403 [==============================] - 54s 134ms/step - loss: 0.2398 - accuracy: 0.9299 - val_loss: 0.9714 - val_accuracy: 0.7457\n",
      "Epoch 7/50\n",
      "389/403 [===========================>..] - ETA: 1s - loss: 0.2044 - accuracy: 0.9403"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-bce8baa68237>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             model.fit(Xtrain, train_y, batch_size=50, epochs=50, verbose=1, \n\u001b[0m\u001b[0;32m     55\u001b[0m                       callbacks=[callbacks], validation_data=(Xtest, test_y))\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['tanh']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4]\n",
    "\n",
    "columns = ['Activation', 'Filters', 'Acc']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# # Separate the sentences and the labels\n",
    "# train_x = list(corpus[corpus.split=='train'].sentence)\n",
    "# train_y = np.array(corpus[corpus.split=='train'].label)\n",
    "# test_x = list(corpus[corpus.split=='test'].sentence)\n",
    "# test_y = np.array(corpus[corpus.split=='test'].label)\n",
    "\n",
    "exp = 0\n",
    "\n",
    "for activation in activations:\n",
    "\n",
    "    for kernel_size in kernel_sizes:\n",
    "        \n",
    "        exp+=1\n",
    "        print('-------------------------------------------')\n",
    "        print('Training {}: {} activation, {} kernel size.'.format(exp, activation, kernel_size))\n",
    "        print('-------------------------------------------')\n",
    "        \n",
    "        # encode data using\n",
    "        # Cleaning and Tokenization\n",
    "        tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "        tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "        # Turn the text into sequence\n",
    "        training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "        test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "        max_len = max_length(training_sequences)\n",
    "\n",
    "        # Pad the sequence to have the same size\n",
    "        Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "        Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "        vocab_size = len(word_index)+1\n",
    "\n",
    "        # Define the input shape\n",
    "        model = define_model(kernel_size, activation, input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "        # Train the model and initialize test accuracy with 0\n",
    "        acc = 0\n",
    "        while(acc<0.7):\n",
    "\n",
    "            model.fit(Xtrain, train_y, batch_size=50, epochs=50, verbose=1, \n",
    "                      callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "            print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "            if (acc<0.6):\n",
    "                print('The model suffered from local minimum. Retrain the model!')\n",
    "                model = define_model(kernel_size, activation, input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "            else:\n",
    "                print('Done!')\n",
    "\n",
    "        parameters = [activation, kernel_size]\n",
    "        entries = parameters + [acc]\n",
    "\n",
    "        temp = pd.DataFrame([entries], columns=columns)\n",
    "        record = record.append(temp, ignore_index=True)\n",
    "        print()\n",
    "        print(record)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>Filters</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tanh</td>\n",
       "      <td>4</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>2</td>\n",
       "      <td>0.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>6</td>\n",
       "      <td>0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>6</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activation Filters    Acc\n",
       "2        relu       3  0.900\n",
       "9        tanh       4  0.898\n",
       "3        relu       4  0.894\n",
       "7        tanh       2  0.892\n",
       "4        relu       5  0.890\n",
       "1        relu       2  0.888\n",
       "5        relu       6  0.886\n",
       "0        relu       1  0.882\n",
       "6        tanh       1  0.880\n",
       "8        tanh       3  0.880\n",
       "11       tanh       6  0.866\n",
       "10       tanh       5  0.864"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.sort_values(by='Acc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record.sort_values(by='Acc', ascending=False)\n",
    "report = report.to_excel('TCN_TREC.xlsx', sheet_name='random')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
