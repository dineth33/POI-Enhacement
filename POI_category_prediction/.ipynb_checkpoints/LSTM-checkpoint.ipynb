{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU2pVpwQeAwi"
   },
   "source": [
    "# LSTM Classification with TREC Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using LSTM model on the TREC Dataset. \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1592,
     "status": "ok",
     "timestamp": 1613881735526,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "co02_OrOeAxK",
    "outputId": "b73caa50-9bb1-4383-a5eb-9ce553cabb9d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import time\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIaHVCBveAxM"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 2615,
     "status": "ok",
     "timestamp": 1613881784144,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "S6pw277beAxN",
    "outputId": "36745604-7817-49cf-9000-baabc84bb554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5952, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>who was the 22nd president of the us ?</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>what is the money they use in zambia ?</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>how many feet in a mile ?</td>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>what is the birthstone of october ?</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>what is e coli ?</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5952 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     how did serfdom develop in and then leave russ...      0  train\n",
       "1      what films featured the character popeye doyle ?      1  train\n",
       "2     how can i find a list of celebrities ' real na...      0  train\n",
       "3     what fowl grabs the spotlight after the chines...      1  train\n",
       "4                       what is the full form of .com ?      2  train\n",
       "...                                                 ...    ...    ...\n",
       "5947             who was the 22nd president of the us ?      3   test\n",
       "5948             what is the money they use in zambia ?      1   test\n",
       "5949                          how many feet in a mile ?      5   test\n",
       "5950                what is the birthstone of october ?      1   test\n",
       "5951                                   what is e coli ?      0   test\n",
       "\n",
       "[5952 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/TREC/TREC.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2660,
     "status": "ok",
     "timestamp": 1613881796000,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "ardMFHDJeAxQ",
    "outputId": "4036087a-056d-4803-e294-e7cbbe754c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5952 entries, 0 to 5951\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  5952 non-null   object\n",
      " 1   label     5952 non-null   int32 \n",
      " 2   split     5952 non-null   object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 116.4+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 2502,
     "status": "ok",
     "timestamp": 1613881796004,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "4_lhbpYGeAxS",
    "outputId": "19c8839e-4422-4108-f47a-cd252d309a1e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentence\n",
       "split label          \n",
       "test  0           138\n",
       "      1            94\n",
       "      2             9\n",
       "      3            65\n",
       "      4            81\n",
       "      5           113\n",
       "train 0          1162\n",
       "      1          1250\n",
       "      2            86\n",
       "      3          1223\n",
       "      4           835\n",
       "      5           896"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by=['split','label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 2302,
     "status": "ok",
     "timestamp": 1613881798729,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "2jwK-HoaeAxT",
    "outputId": "b773f7ef-c31f-4796-a73a-e973f3e22cf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>5452</td>\n",
       "      <td>5452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  label\n",
       "split                 \n",
       "test        500    500\n",
       "train      5452   5452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby(by='split').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2596,
     "status": "ok",
     "timestamp": 1613881801371,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "-q4r3grgeAxT",
    "outputId": "a038241c-8238-4eb6-892c-16571b98cfbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5452\n",
      "5452\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "# Separate the sentences and the labels for training and testing\n",
    "train_x = list(corpus[corpus.split=='train'].sentence)\n",
    "train_y = np.array(corpus[corpus.split=='train'].label)\n",
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "\n",
    "test_x = list(corpus[corpus.split=='test'].sentence)\n",
    "test_y = np.array(corpus[corpus.split=='test'].label)\n",
    "print(len(test_x))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJV8dGfzeAxX"
   },
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>## Develop Vocabulary\n",
    "\n",
    "A part of preparing text for text classification involves defining and tailoring the vocabulary of words supported by the model. **We can do this by loading all of the documents in the dataset and building a set of words.**\n",
    "\n",
    "The larger the vocabulary, the more sparse the representation of each word or document. So, we may decide to support all of these words, or perhaps discard some. The final chosen vocabulary can then be saved to a file for later use, such as filtering words in new documents in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1613881803995,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "8ozAFDk3eAxe"
   },
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1650,
     "status": "ok",
     "timestamp": 1613881808491,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "fWW5xE8peAxl",
    "outputId": "e29c9383-98b8-4e99-b2f3-85f7c62fbda1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  what is the full form of .com ?\n",
      "Into a sequence of int: [3, 4, 2, 471, 261, 5, 372]\n",
      "Into a padded sequence: [  3   4   2 471 261   5 372   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "train_x = list(corpus[corpus.split=='train'].sentence)\n",
    "train_y = np.array(corpus[corpus.split=='train'].label)\n",
    "test_x = list(corpus[corpus.split=='test'].sentence)\n",
    "test_y = np.array(corpus[corpus.split=='test'].label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "print(\"Example of sentence: \", train_x[4])\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1586,
     "status": "ok",
     "timestamp": 1613881812377,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "pO2INZaHeAxm",
    "outputId": "09eb9a33-b69b-432e-c86a-5e63d59e022c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "what 3\n",
      "is 4\n",
      "of 5\n",
      "in 6\n",
      "a 7\n",
      "how 8\n",
      "'s 9\n",
      "was 10\n",
      "8461\n"
     ]
    }
   ],
   "source": [
    "# See the first 10 words in the vocabulary\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odRK28LIeAxn"
   },
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1613883204563,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "LYbWCxixeAxx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=6, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1613883207761,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "aOwVZzKkeAxy",
    "outputId": "a921d83c-3d4a-4a0a-f963-06ae5c9e7472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 586,470\n",
      "Trainable params: 586,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1613883808199,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "n9AjiIuBeAxz"
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=20, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)\n",
    "# callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKzlhfLjljQR"
   },
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 82232,
     "status": "error",
     "timestamp": 1613883893355,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "a5s8LOGseAxz",
    "outputId": "93a04a62-5f72-48f9-de12-7bc52a78913f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "110/110 [==============================] - 47s 196ms/step - loss: 1.4642 - accuracy: 0.3943 - val_loss: 0.6511 - val_accuracy: 0.8220\n",
      "Epoch 2/60\n",
      "110/110 [==============================] - 15s 137ms/step - loss: 0.3279 - accuracy: 0.9124 - val_loss: 0.4272 - val_accuracy: 0.8780\n",
      "Epoch 3/60\n",
      "110/110 [==============================] - 14s 130ms/step - loss: 0.1063 - accuracy: 0.9735 - val_loss: 0.4817 - val_accuracy: 0.8820\n",
      "Epoch 4/60\n",
      "110/110 [==============================] - 16s 143ms/step - loss: 0.0375 - accuracy: 0.9899 - val_loss: 0.5721 - val_accuracy: 0.8640\n",
      "Epoch 5/60\n",
      "110/110 [==============================] - 15s 136ms/step - loss: 0.0255 - accuracy: 0.9951 - val_loss: 0.5366 - val_accuracy: 0.8960\n",
      "Epoch 6/60\n",
      "110/110 [==============================] - 14s 129ms/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 0.6522 - val_accuracy: 0.8620\n",
      "Epoch 7/60\n",
      "110/110 [==============================] - 15s 139ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.7505 - val_accuracy: 0.8580\n",
      "Epoch 8/60\n",
      "110/110 [==============================] - 14s 130ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.7335 - val_accuracy: 0.8740\n",
      "Epoch 9/60\n",
      "110/110 [==============================] - 16s 149ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.8155 - val_accuracy: 0.8660\n",
      "Epoch 10/60\n",
      "110/110 [==============================] - 15s 139ms/step - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.7250 - val_accuracy: 0.8640\n",
      "Epoch 11/60\n",
      "110/110 [==============================] - 16s 142ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.8077 - val_accuracy: 0.8560\n",
      "Epoch 12/60\n",
      "110/110 [==============================] - 15s 140ms/step - loss: 0.0257 - accuracy: 0.9917 - val_loss: 0.8326 - val_accuracy: 0.8420\n",
      "Epoch 13/60\n",
      "110/110 [==============================] - 14s 128ms/step - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.7212 - val_accuracy: 0.8600\n",
      "Epoch 14/60\n",
      "110/110 [==============================] - 14s 129ms/step - loss: 0.0158 - accuracy: 0.9957 - val_loss: 0.8657 - val_accuracy: 0.8420\n",
      "Epoch 15/60\n",
      "110/110 [==============================] - 14s 131ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.8647 - val_accuracy: 0.8500\n",
      "Epoch 16/60\n",
      "110/110 [==============================] - 14s 127ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.8711 - val_accuracy: 0.8420\n",
      "Epoch 17/60\n",
      "110/110 [==============================] - 14s 127ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.8350 - val_accuracy: 0.8620\n",
      "Epoch 18/60\n",
      "110/110 [==============================] - 14s 128ms/step - loss: 8.7622e-04 - accuracy: 0.9999 - val_loss: 0.9567 - val_accuracy: 0.8440\n",
      "Epoch 19/60\n",
      "110/110 [==============================] - 14s 129ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.9333 - val_accuracy: 0.8520\n",
      "Epoch 20/60\n",
      "110/110 [==============================] - 14s 127ms/step - loss: 7.2234e-04 - accuracy: 0.9999 - val_loss: 0.9318 - val_accuracy: 0.8520\n",
      "Epoch 21/60\n",
      "110/110 [==============================] - 14s 127ms/step - loss: 4.6954e-04 - accuracy: 1.0000 - val_loss: 0.9337 - val_accuracy: 0.8500\n",
      "Epoch 22/60\n",
      "110/110 [==============================] - 16s 143ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.9607 - val_accuracy: 0.8480\n",
      "Epoch 23/60\n",
      "110/110 [==============================] - 16s 141ms/step - loss: 5.8650e-04 - accuracy: 0.9998 - val_loss: 0.9675 - val_accuracy: 0.8480\n",
      "Epoch 24/60\n",
      "110/110 [==============================] - 14s 131ms/step - loss: 2.3815e-04 - accuracy: 1.0000 - val_loss: 0.9810 - val_accuracy: 0.8480\n",
      "Epoch 25/60\n",
      "110/110 [==============================] - 15s 132ms/step - loss: 3.3722e-04 - accuracy: 0.9999 - val_loss: 1.0069 - val_accuracy: 0.8500\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "Test Accuracy: 89.60000276565552\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "train_x = list(corpus[corpus.split=='train'].sentence)\n",
    "train_y = np.array(corpus[corpus.split=='train'].label)\n",
    "test_x = list(corpus[corpus.split=='test'].sentence)\n",
    "test_y = np.array(corpus[corpus.split=='test'].label)\n",
    "\n",
    "\n",
    "# encode data using\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "# Define the input shape\n",
    "model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "# Train the model\n",
    "model.fit(Xtrain, train_y, batch_size=50, epochs=60, verbose=1, \n",
    "          callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "print('Test Accuracy: {}'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_RMogwG67hY"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiUaxK0aykWU"
   },
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4pUgiWpyutx"
   },
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5qt581_y1Q_"
   },
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 75542,
     "status": "ok",
     "timestamp": 1613881905874,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "awd7t-EdtGzb"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74169,
     "status": "ok",
     "timestamp": 1613881905881,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "jFKze3SHtG4Y",
    "outputId": "4d7c17a1-4002-4890-ca5f-e212055e2b64",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfs__-mLzRAI"
   },
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 71200,
     "status": "ok",
     "timestamp": 1613881905883,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "mOBPMjI2tG8a"
   },
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68950,
     "status": "ok",
     "timestamp": 1613881905884,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "bpd4CXWdtHAn",
    "outputId": "cbac8ec2-fb33-4204-8608-351d38e7a994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7526 words present from 8761 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dk90wzj20A5"
   },
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean:  -0.003527845\n",
      "emb_std:  0.13315111\n"
     ]
    }
   ],
   "source": [
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "print('emb_mean: ', emb_mean)\n",
    "print('emb_std: ', emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 897,
     "status": "ok",
     "timestamp": 1613881918001,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "KN8eakpJ21TV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1613881920544,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "W7Mq_4fE21sO",
    "outputId": "43400d9a-9f0d-45f0-b3a1-01559f238b2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
       "        -0.09975549, -0.08595189],\n",
       "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
       "        -0.13156594, -0.05996158],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2OTr-Rm3QLP"
   },
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1613882514602,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "s-2rqTcf3UHq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=6, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1613882166955,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "x0xGZJsx3Ud0",
    "outputId": "c21cd79b-0239-4288-889a-c486500d2d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 586,470\n",
      "Trainable params: 286,470\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9lxRUZc3cZp"
   },
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1613884001572,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "2PAHGhxb3b4A"
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=20, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1457608,
     "status": "ok",
     "timestamp": 1613885460118,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "UBgOBSen3fbO",
    "outputId": "7f4f36ad-f3a3-427f-89ec-29351db68ef2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 18s 96ms/step - loss: 1.5372 - accuracy: 0.3539 - val_loss: 0.8725 - val_accuracy: 0.7380\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 7s 66ms/step - loss: 0.7392 - accuracy: 0.7341 - val_loss: 0.5439 - val_accuracy: 0.8380\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 7s 66ms/step - loss: 0.4992 - accuracy: 0.8333 - val_loss: 0.4462 - val_accuracy: 0.8720\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 7s 66ms/step - loss: 0.3982 - accuracy: 0.8683 - val_loss: 0.3193 - val_accuracy: 0.9160\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.3350 - accuracy: 0.8953 - val_loss: 0.3201 - val_accuracy: 0.8960\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.3144 - accuracy: 0.8925 - val_loss: 0.3446 - val_accuracy: 0.8800\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.2396 - accuracy: 0.9228 - val_loss: 0.4014 - val_accuracy: 0.8580\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.2163 - accuracy: 0.9296 - val_loss: 0.4261 - val_accuracy: 0.8540\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.1783 - accuracy: 0.9398 - val_loss: 0.3415 - val_accuracy: 0.8900\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.1355 - accuracy: 0.9617 - val_loss: 0.3172 - val_accuracy: 0.8800\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.1228 - accuracy: 0.9601 - val_loss: 0.3405 - val_accuracy: 0.9080\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.1099 - accuracy: 0.9676 - val_loss: 0.4012 - val_accuracy: 0.8740\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 8s 69ms/step - loss: 0.0771 - accuracy: 0.9793 - val_loss: 0.3477 - val_accuracy: 0.9080\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.1834 - accuracy: 0.9423 - val_loss: 0.3412 - val_accuracy: 0.9100\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 8s 70ms/step - loss: 0.0686 - accuracy: 0.9820 - val_loss: 0.3425 - val_accuracy: 0.9020\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 8s 70ms/step - loss: 0.0526 - accuracy: 0.9820 - val_loss: 0.4079 - val_accuracy: 0.8920\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 8s 69ms/step - loss: 0.1117 - accuracy: 0.9645 - val_loss: 0.3723 - val_accuracy: 0.9080\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.0381 - accuracy: 0.9867 - val_loss: 0.4278 - val_accuracy: 0.8840\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 0.3661 - val_accuracy: 0.9040\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.4618 - val_accuracy: 0.8880\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 8s 68ms/step - loss: 0.0251 - accuracy: 0.9942 - val_loss: 0.4494 - val_accuracy: 0.8880\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 8s 68ms/step - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.4609 - val_accuracy: 0.8900\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.0766 - accuracy: 0.9768 - val_loss: 0.4082 - val_accuracy: 0.9060\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.0155 - accuracy: 0.9971 - val_loss: 0.6686 - val_accuracy: 0.8600\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Test Accuracy: 91.60000085830688\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "train_x = list(corpus[corpus.split=='train'].sentence)\n",
    "train_y = np.array(corpus[corpus.split=='train'].label)\n",
    "test_x = list(corpus[corpus.split=='test'].sentence)\n",
    "test_y = np.array(corpus[corpus.split=='test'].label)\n",
    "\n",
    "\n",
    "# encode data using\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "# Define the input shape\n",
    "model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "# Train the model\n",
    "model.fit(Xtrain, train_y, batch_size=50, epochs=100, verbose=1, \n",
    "          callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "print('Test Accuracy: {}'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KPnmNi--jBK"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzHRdT3xCrVO"
   },
   "source": [
    "# Model 3: Word2Vec Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZptM1m0C4j2"
   },
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x23ReO7fC5gW"
   },
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1648,
     "status": "ok",
     "timestamp": 1613885575617,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "vltDrSwlCviE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=6, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1613885584226,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "ORayWvkPEAeX",
    "outputId": "cd1727ad-b0de-47eb-82a1-8cfdac702225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 586,470\n",
      "Trainable params: 586,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wyz3lOreDAqC"
   },
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1613885591367,
     "user": {
      "displayName": "Diardano Raihan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7uiez3sFTDx1VaTwg2sO7Gvixa7HqFQO4k5j=s64",
      "userId": "15266981897338202066"
     },
     "user_tz": -480
    },
    "id": "rTsFU0aOCvys"
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=20, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 21s 116ms/step - loss: 1.4518 - accuracy: 0.3906 - val_loss: 0.5454 - val_accuracy: 0.8500\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 11s 98ms/step - loss: 0.4358 - accuracy: 0.8628 - val_loss: 0.3900 - val_accuracy: 0.8840\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 11s 96ms/step - loss: 0.1782 - accuracy: 0.9537 - val_loss: 0.5386 - val_accuracy: 0.8200\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 11s 96ms/step - loss: 0.0821 - accuracy: 0.9830 - val_loss: 0.5675 - val_accuracy: 0.8380\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 11s 96ms/step - loss: 0.0947 - accuracy: 0.9762 - val_loss: 0.5656 - val_accuracy: 0.8540\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 11s 97ms/step - loss: 0.0233 - accuracy: 0.9948 - val_loss: 0.5063 - val_accuracy: 0.8780\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 11s 97ms/step - loss: 0.0225 - accuracy: 0.9951 - val_loss: 0.6233 - val_accuracy: 0.8820\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 11s 97ms/step - loss: 0.0172 - accuracy: 0.9971 - val_loss: 0.7384 - val_accuracy: 0.8420\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 11s 97ms/step - loss: 0.0340 - accuracy: 0.9915 - val_loss: 0.9169 - val_accuracy: 0.8320\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 11s 97ms/step - loss: 0.0240 - accuracy: 0.9949 - val_loss: 0.7248 - val_accuracy: 0.8480\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 11s 97ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.7626 - val_accuracy: 0.8500\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 11s 98ms/step - loss: 0.0046 - accuracy: 0.9996 - val_loss: 0.7261 - val_accuracy: 0.8660\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 11s 98ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 0.7919 - val_accuracy: 0.8580\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 11s 97ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.8240 - val_accuracy: 0.8600\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 11s 99ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.8826 - val_accuracy: 0.8580\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 11s 99ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.8610 - val_accuracy: 0.8580\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 11s 100ms/step - loss: 7.6271e-04 - accuracy: 1.0000 - val_loss: 0.9157 - val_accuracy: 0.8600\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 11s 100ms/step - loss: 6.6618e-04 - accuracy: 0.9998 - val_loss: 0.9601 - val_accuracy: 0.8600\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 11s 100ms/step - loss: 5.0951e-04 - accuracy: 0.9998 - val_loss: 0.9748 - val_accuracy: 0.8600\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 11s 99ms/step - loss: 3.0202e-04 - accuracy: 1.0000 - val_loss: 1.0169 - val_accuracy: 0.8580\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 11s 99ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 1.0200 - val_accuracy: 0.8540\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 11s 98ms/step - loss: 2.5368e-04 - accuracy: 1.0000 - val_loss: 1.0370 - val_accuracy: 0.8600\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Test Accuracy: 88.40000033378601\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "train_x = list(corpus[corpus.split=='train'].sentence)\n",
    "train_y = np.array(corpus[corpus.split=='train'].label)\n",
    "test_x = list(corpus[corpus.split=='test'].sentence)\n",
    "test_y = np.array(corpus[corpus.split=='test'].label)\n",
    "\n",
    "\n",
    "# encode data using\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "# Define the input shape\n",
    "model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "# Train the model\n",
    "model.fit(Xtrain, train_y, batch_size=50, epochs=100, verbose=1, \n",
    "          callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "print('Test Accuracy: {}'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXNADu3GEtB0"
   },
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_TREC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
